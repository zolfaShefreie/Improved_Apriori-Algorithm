{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "improvedAprioriAlgorithm.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "V3VnJUA3Jl5h",
        "HNJ8ILrVPAvn",
        "Z3T7RyJdOkNp",
        "E_-E6JqZP07N"
      ],
      "authorship_tag": "ABX9TyPf3Mo6uG/a8GYYu07PHEgR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zolfaShefreie/Improved_Apriori-Algorithm/blob/main/improvedAprioriAlgorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#download datasets"
      ],
      "metadata": {
        "id": "V3VnJUA3Jl5h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j-Q-3kBtJZtm"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groceries_url = 'https://raw.githubusercontent.com/zolfaShefreie/Improved_Apriori-Algorithm/main/groceries.csv'\n",
        "groceries_file_path = \"groceries.csv\""
      ],
      "metadata": {
        "id": "rRbFhyIfKF7H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file(url: str, file_path):\n",
        "  \"\"\"\n",
        "  download file and save on file path\n",
        "  \"\"\"\n",
        "  file_content = requests.get(url).text\n",
        "  file = open(file_path, 'w')\n",
        "  file.write(file_content)\n",
        "  file.close()"
      ],
      "metadata": {
        "id": "TOHB6WlZKwIV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_file(groceries_url, groceries_file_path)"
      ],
      "metadata": {
        "id": "kBfepExrK0BK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "HNJ8ILrVPAvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from multiprocessing import Pool\n",
        "import time"
      ],
      "metadata": {
        "id": "EwfgytGSPCek"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Useful methods"
      ],
      "metadata": {
        "id": "Z3T7RyJdOkNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_csv_to_set(path: str) -> set:\n",
        "    \"\"\"\n",
        "    this function convert a csv file to set for get all values of the table\n",
        "    :param path: the path of a file with .csv format\n",
        "    :return: a set of values without np.nan\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, header=None)\n",
        "\n",
        "    np_array = df.to_numpy()\n",
        "    np_array = np_array.flatten().tolist()\n",
        "    items = set(np_array)\n",
        "    items.discard(np.nan)\n",
        "\n",
        "    return items"
      ],
      "metadata": {
        "id": "b1PwHI3_OtAN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_csv_to_dict_data(path: str) -> dict:\n",
        "    \"\"\"\n",
        "    this function convert a csv file to dict\n",
        "    :param path: the path of a file with .csv format\n",
        "    :return: return  a dictionary with key: index and value a set of sell items without np.nan\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    dict_data = df.T.to_dict('list')\n",
        "    for each in dict_data:\n",
        "        set_data = set(dict_data[each])\n",
        "        set_data.discard(np.nan)\n",
        "        dict_data[each] = set_data\n",
        "\n",
        "    return dict_data"
      ],
      "metadata": {
        "id": "f7VEPHrzPOe1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_csv_to_list_tuple_data(path: str, element_1_type=list, elemetn_2_type=list) -> list:\n",
        "    \"\"\"\n",
        "    this function convert a csv file to a list of tuple\n",
        "    :param path: the path of a file with .csv format\n",
        "    :param element_1_type: format of first element of tuple. the format must be itreable\n",
        "    :param elemetn_2_type: format of second element of tuple. the format must be itreable\n",
        "    :return: return  a list with key: index and value a list of sell items without np.nan\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    dict_data = df.T.to_dict('list')\n",
        "    tuple_list = list()\n",
        "    for key, value in dict_data.items():\n",
        "        values = set(value)\n",
        "        values.discard(np.nan)\n",
        "        tuple_list.append((element_1_type([key]), elemetn_2_type(values)))\n",
        "\n",
        "    return tuple_list"
      ],
      "metadata": {
        "id": "PC1Yv36OZU-H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic Apriori"
      ],
      "metadata": {
        "id": "E_-E6JqZP07N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Rule:\n",
        "    def __init__(self, rule_part_a: dict, rule_part_b: dict, a_plus_b: int, max_trans: int):\n",
        "        self.rule_part_a = rule_part_a\n",
        "        self.rule_part_b = rule_part_b\n",
        "        self.max_transactions = max_trans\n",
        "        self.sup_a_plus_b = a_plus_b\n",
        "        self.sup = self.calculate_sup(a_plus_b, max_trans)\n",
        "        self.conf = self.calculate_conf(list(rule_part_a.values())[0], a_plus_b)\n",
        "        self.lift = self.calculate_lift(self.conf, list(rule_part_b.values())[0] / max_trans)\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_sup(sup_count_a_plus_b: int, max_trans: int):\n",
        "        return sup_count_a_plus_b / max_trans\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_conf(sup_count_a: int, sup_count_a_plus_b: int):\n",
        "        return sup_count_a_plus_b / sup_count_a\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_lift(conf: float, sup_b: float):\n",
        "        return conf / sup_b\n",
        "\n",
        "    def __le__(self, other):\n",
        "        if self.rule_part_a < other.rule_part_a:\n",
        "            return True\n",
        "        return self.rule_part_a == other.rule_part_a and self.rule_part_b < other.rule_part_b\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if self.rule_part_a == other.rule_part_a and self.rule_part_b == other.rule_part_b:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"{}->{} support: {}, lift: {}, confidence: {}\".\\\n",
        "            format(list(list(self.rule_part_a.keys())[0]), list(list(self.rule_part_b.keys())[0]),\n",
        "                   self.sup, self.lift, self.conf)\n",
        "\n",
        "    @staticmethod\n",
        "    def sort_by(elem, sort_by):\n",
        "        if sort_by == \"lift\":\n",
        "            return elem.lift\n",
        "        if sort_by == \"support\":\n",
        "            return elem.sup\n",
        "        elif sort_by == \"confidence\":\n",
        "            return elem.conf\n",
        "\n",
        "        return elem.lift"
      ],
      "metadata": {
        "id": "KzcO7f__Pz_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Arules:\n",
        "    MAX_LENGTH = 1000\n",
        "    MAX_ITEM_SET_RESULT = 10\n",
        "\n",
        "    def __init__(self):\n",
        "        self.max_transactions = 0\n",
        "        self.frequents = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_items(transactions: dict) -> set:\n",
        "        \"\"\"\n",
        "        get the items of transactions\n",
        "        :param transactions: a dict {transaction_id: set of items}\n",
        "        :return: set of items\n",
        "        \"\"\"\n",
        "        items = set()\n",
        "        for each in transactions.values():\n",
        "            items.update(each)\n",
        "        return set(items)\n",
        "\n",
        "    def level_process(self, transactions: dict, level: int, min_sup: float):\n",
        "        \"\"\"\n",
        "        a func management process of one level\n",
        "        :param transactions:\n",
        "        :param level: level or depth of process\n",
        "        :param min_sup: minimum support\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        item_keys = self.get_level_item_keys(level)\n",
        "        args = [(dict(list(transactions.items())[i*self.MAX_LENGTH: (i+1)*self.MAX_LENGTH]), level, item_keys)\n",
        "                for i in range(int(len(transactions) / self.MAX_LENGTH) + 1)]\n",
        "        pool = Pool(int(len(transactions) / self.MAX_LENGTH) + 1)\n",
        "        results = pool.starmap(self.get_c_dict, args)\n",
        "        pool.close()\n",
        "        pool.terminate()\n",
        "        c_results = self.merge_dicts(results)\n",
        "        if level == 1:\n",
        "            self.frequents.append(self.get_l_dict(len(transactions), c_results, min_sup, level))\n",
        "        else:\n",
        "            result = self.get_l_dict(len(transactions), c_results, min_sup, level, self.frequents[level - 2])\n",
        "            self.frequents.append(result)\n",
        "\n",
        "    def get_level_item_keys(self, level: int):\n",
        "        \"\"\"\n",
        "        :param level: level or depth of process\n",
        "        :return: the key items\n",
        "        \"\"\"\n",
        "        key_list = None\n",
        "        if len(self.frequents) > 0:\n",
        "            items = list(self.frequents[level - 2].keys())\n",
        "            key_list = set()\n",
        "            count = 1\n",
        "            for i in range(len(items)):\n",
        "                item = sorted(list(items[i]))\n",
        "                for each in items[i + 1:]:\n",
        "                    each = sorted(list(each))\n",
        "                    if item[:-1] == each[:-1] and item[-1] != each[-1]:\n",
        "                        key_list.add(frozenset(item + [each[-1], ]))\n",
        "                    count += 1\n",
        "        return key_list\n",
        "\n",
        "    def get_c_dict(self, transactions: dict, level: int, item_keys=None) -> dict:\n",
        "        \"\"\"\n",
        "        make the table c\n",
        "        :param transactions: the dict of transactions\n",
        "        :param level: level or depth of process\n",
        "        :param item_keys: a list of tuple for a set of items\n",
        "        :return: a dict {item: sup}\n",
        "        \"\"\"\n",
        "        if level == 1:\n",
        "            items = self.get_items(transactions)\n",
        "            item_keys = list(map(frozenset, itertools.combinations(set(items), level)))\n",
        "\n",
        "        result_dict = dict()\n",
        "        for each in item_keys:\n",
        "            for transaction in transactions.values():\n",
        "                if transaction.intersection(each) == set(each):\n",
        "                    result_dict[each] = result_dict.get(each, 0) + 1\n",
        "        return result_dict\n",
        "\n",
        "    @staticmethod\n",
        "    def get_l_dict(max_length: int, c: dict, min_sup: float, level: int, pre_l=None) -> dict:\n",
        "        \"\"\"\n",
        "        make the table l\n",
        "        :param max_length: number of transactions\n",
        "        :param c: the return of get_c_dict\n",
        "        :param min_sup: a float number between 0 and 1\n",
        "        :param level: level or depth of process\n",
        "        :param pre_l: a dict for self.c[level-1]\n",
        "        :return: a dict with valid sup\n",
        "        \"\"\"\n",
        "        c_copy = dict(c)\n",
        "        for each in c:\n",
        "            if len(each) > 1:\n",
        "                sub_keys = list(map(frozenset, itertools.combinations(set(each), level-1)))\n",
        "                if pre_l is None:\n",
        "                    raise Exception\n",
        "                pre_keys = set(pre_l.keys())\n",
        "                commons = pre_keys.intersection(sub_keys)\n",
        "                if len(commons) != len(sub_keys):\n",
        "                    c_copy.pop(each)\n",
        "                    continue\n",
        "            if c[each] / max_length < min_sup:\n",
        "                c_copy.pop(each)\n",
        "        return c_copy\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_dicts(list_dict: list) -> dict:\n",
        "        \"\"\"\n",
        "        if a key in dict_a and dict_b => value of key = dict_b[key] + dict_a[key]\n",
        "        :param list_dict:  list of {key: value} that value must be int\n",
        "        :return: merge dict\n",
        "        \"\"\"\n",
        "        result = dict()\n",
        "        keys = list(set([frozenset(key) for each in list_dict for key in each]))\n",
        "        for key in keys:\n",
        "            value = 0\n",
        "            for each in list_dict:\n",
        "                value += each.get(key, 0)\n",
        "            result[key] = value\n",
        "        return result\n",
        "\n",
        "    def get_frequent_item_sets(self, transactions: dict, min_sup=float('-inf')) -> list:\n",
        "        \"\"\"\n",
        "        get n item set\n",
        "        :param transactions: a dict of transaction ids and the list of items\n",
        "        :param min_sup: minimum support\n",
        "        :return: the list of last level\n",
        "        \"\"\"\n",
        "        self.max_transactions = len(transactions)\n",
        "        level = 1\n",
        "        while True:\n",
        "            self.level_process(transactions, level, min_sup)\n",
        "            if not self.frequents[level - 1]:\n",
        "                break\n",
        "            level += 1\n",
        "        return list(self.frequents[level - 2].keys())[:self.MAX_ITEM_SET_RESULT]\n",
        "\n",
        "    def get_arules(self, min_sup=float('-inf'), min_conf=float('-inf'), min_lift=float('-inf'), sort_by='lift'):\n",
        "        \"\"\"\n",
        "        get all rules and sort it\n",
        "        :param min_sup: a float between 0 and 1\n",
        "        :param min_conf: a float between 0 and 1\n",
        "        :param min_lift: a float between 0 and 1\n",
        "        :param sort_by: choices = (lift, support, confidence)\n",
        "        :return: sorted rules\n",
        "        \"\"\"\n",
        "        item_sets = list(self.frequents[len(self.frequents) - 2].keys())\n",
        "        args = [(set(each), self.frequents[len(self.frequents) - 2][each], min_sup, min_conf, min_lift) for each in item_sets]\n",
        "        pool = Pool(len(item_sets))\n",
        "        rules = pool.starmap(self.get_item_set_rule, args)\n",
        "        pool.close()\n",
        "        pool.terminate()\n",
        "        rules = [rule for each in rules for rule in each]\n",
        "        return sorted(rules, key=lambda x: Rule.sort_by(x, sort_by), reverse=True)\n",
        "\n",
        "    def get_item_set_rule(self, item_set: set, sup_count: int, min_sup=float('-inf'), min_conf=float('-inf'),\n",
        "                          min_lift=float('-inf')) -> list:\n",
        "        \"\"\"\n",
        "        get unsorted rules of one item set\n",
        "        :param item_set: one set of  frequent item\n",
        "        :param sup_count: the sup count of item_set in transactions\n",
        "        :param min_sup: a float between 0 and 1\n",
        "        :param min_conf: a float between 0 and 1\n",
        "        :param min_lift: a float between 0 and 1\n",
        "        :return: a list of rule\n",
        "        \"\"\"\n",
        "        rule_parts_list = list()\n",
        "        rule_obj_list = list()\n",
        "        for i in range(int(len(item_set)-1)):\n",
        "            sub_sets = list(map(set, itertools.combinations(item_set, i+1)))\n",
        "            for each in sub_sets:\n",
        "                complement = item_set - each\n",
        "                if (each, complement) not in rule_parts_list:\n",
        "                    rule_parts_list.append((each, complement))\n",
        "                    rule = Rule(rule_part_a={frozenset(each): self.frequents[len(each) - 1][frozenset(each)]},\n",
        "                                rule_part_b={frozenset(complement): self.frequents[len(complement) - 1][frozenset(complement)]},\n",
        "                                a_plus_b=sup_count,\n",
        "                                max_trans=self.max_transactions)\n",
        "                    if rule.sup >= min_sup and rule.conf >= min_conf and rule.lift >= min_lift:\n",
        "                        rule_obj_list.append(rule)\n",
        "        return rule_obj_list"
      ],
      "metadata": {
        "id": "HDN38gmLQAOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##analyse results"
      ],
      "metadata": {
        "id": "GBBjNea2Qkkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactionss = convert_csv_to_dict_data(groceries_file_path)\n",
        "algo = Arules()\n",
        "frequents = algo.get_frequent_item_sets(transactionss, 0.005)\n",
        "\n",
        "l = algo.get_arules(min_sup=0.005, min_conf=0.2)\n",
        "for each in l:\n",
        "  print(str(each))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D36lH9wAQJsK",
        "outputId": "07772d50-c9a4-43d2-c817-a44ff861aa45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['root vegetables', 'yogurt']->['tropical fruit', 'whole milk'] support: 0.0056939501779359435, lift: 5.212371290127195, confidence: 0.2204724409448819\n",
            "['root vegetables', 'tropical fruit']->['yogurt', 'whole milk'] support: 0.0056939501779359435, lift: 4.828813663343766, confidence: 0.27053140096618356\n",
            "['root vegetables', 'pip fruit']->['other vegetables', 'whole milk'] support: 0.005490594814438231, lift: 4.716272378516624, confidence: 0.35294117647058826\n",
            "['root vegetables', 'tropical fruit']->['other vegetables', 'whole milk'] support: 0.007015760040671073, lift: 4.4542572463768115, confidence: 0.3333333333333333\n",
            "['root vegetables', 'citrus fruit']->['other vegetables', 'whole milk'] support: 0.005795627859684799, lift: 4.377459707646177, confidence: 0.3275862068965517\n",
            "['other vegetables', 'fruit/vegetable juice']->['yogurt', 'whole milk'] support: 0.005083884087442806, lift: 4.3114407708426485, confidence: 0.24154589371980675\n",
            "['other vegetables', 'pip fruit']->['root vegetables', 'whole milk'] support: 0.005490594814438231, lift: 4.29625375150667, confidence: 0.21011673151750973\n",
            "['other vegetables', 'citrus fruit']->['root vegetables', 'whole milk'] support: 0.005795627859684799, lift: 4.103796374923136, confidence: 0.2007042253521127\n",
            "['other vegetables', 'whole milk', 'citrus fruit']->['root vegetables'] support: 0.005795627859684799, lift: 4.085492945429104, confidence: 0.4453125\n",
            "['root vegetables', 'whipped/sour cream']->['other vegetables', 'whole milk'] support: 0.005185561769191663, lift: 4.056555706521739, confidence: 0.30357142857142855\n",
            "['root vegetables', 'yogurt']->['other vegetables', 'whole milk'] support: 0.007829181494661922, lift: 4.050918991783636, confidence: 0.3031496062992126\n",
            "['other vegetables', 'tropical fruit']->['yogurt', 'whole milk'] support: 0.007625826131164209, lift: 3.792357958489072, confidence: 0.21246458923512748\n",
            "['yogurt', 'pip fruit']->['other vegetables', 'whole milk'] support: 0.005083884087442806, lift: 3.7747942765905185, confidence: 0.2824858757062147\n",
            "['other vegetables', 'tropical fruit', 'whole milk']->['root vegetables'] support: 0.007015760040671073, lift: 3.7680736940298507, confidence: 0.4107142857142857\n",
            "['root vegetables', 'yogurt', 'whole milk']->['tropical fruit'] support: 0.0056939501779359435, lift: 3.732043150647802, confidence: 0.3916083916083916\n",
            "['other vegetables', 'whole milk', 'pip fruit']->['root vegetables'] support: 0.005490594814438231, lift: 3.724960722702278, confidence: 0.40601503759398494\n",
            "['yogurt', 'fruit/vegetable juice']->['other vegetables', 'whole milk'] support: 0.005083884087442806, lift: 3.631187972589792, confidence: 0.2717391304347826\n",
            "['whipped/sour cream', 'yogurt']->['other vegetables', 'whole milk'] support: 0.005592272496187087, lift: 3.602708066922421, confidence: 0.2696078431372549\n",
            "['other vegetables', 'yogurt', 'whole milk']->['whipped/sour cream'] support: 0.005592272496187087, lift: 3.503513714822371, confidence: 0.2511415525114155\n",
            "['tropical fruit', 'yogurt']->['other vegetables', 'whole milk'] support: 0.007625826131164209, lift: 3.4798884737318843, confidence: 0.2604166666666667\n",
            "['other vegetables', 'whole milk', 'fruit/vegetable juice']->['yogurt'] support: 0.005083884087442806, lift: 3.4797899742421237, confidence: 0.4854368932038835\n",
            "['tropical fruit', 'yogurt', 'whole milk']->['root vegetables'] support: 0.0056939501779359435, lift: 3.4481117900430736, confidence: 0.37583892617449666\n",
            "['rolls/buns', 'root vegetables']->['other vegetables', 'whole milk'] support: 0.0062023385866802234, lift: 3.4105819083136257, confidence: 0.25523012552301255\n",
            "['root vegetables', 'tropical fruit', 'whole milk']->['yogurt'] support: 0.0056939501779359435, lift: 3.4019370460048424, confidence: 0.4745762711864407\n",
            "['root vegetables', 'whole milk', 'citrus fruit']->['other vegetables'] support: 0.005795627859684799, lift: 3.2731651777894553, confidence: 0.6333333333333333\n",
            "['other vegetables', 'yogurt', 'whole milk']->['tropical fruit'] support: 0.007625826131164209, lift: 3.2637119040033977, confidence: 0.3424657534246575\n",
            "['other vegetables', 'whipped/sour cream', 'whole milk']->['root vegetables'] support: 0.005185561769191663, lift: 3.2492809390547266, confidence: 0.3541666666666667\n",
            "['other vegetables', 'yogurt', 'whole milk']->['root vegetables'] support: 0.007829181494661922, lift: 3.225716451986642, confidence: 0.3515981735159817\n",
            "['other vegetables', 'tropical fruit', 'whole milk']->['yogurt'] support: 0.007625826131164209, lift: 3.2001639941690962, confidence: 0.44642857142857145\n",
            "['rolls/buns', 'other vegetables', 'whole milk']->['root vegetables'] support: 0.0062023385866802234, lift: 3.179777603459973, confidence: 0.3465909090909091\n",
            "['root vegetables', 'whole milk', 'pip fruit']->['other vegetables'] support: 0.005490594814438231, lift: 3.1713681746524625, confidence: 0.6136363636363636\n",
            "['other vegetables', 'yogurt', 'whole milk']->['fruit/vegetable juice'] support: 0.005083884087442806, lift: 3.158134725674174, confidence: 0.228310502283105\n",
            "['root vegetables', 'other vegetables', 'whole milk']->['pip fruit'] support: 0.005490594814438231, lift: 3.130836162988115, confidence: 0.23684210526315788\n",
            "['root vegetables', 'other vegetables', 'whole milk']->['whipped/sour cream'] support: 0.005185561769191663, lift: 3.120474057484136, confidence: 0.2236842105263158\n",
            "['root vegetables', 'tropical fruit', 'whole milk']->['other vegetables'] support: 0.007015760040671073, lift: 3.0220570553185424, confidence: 0.5847457627118644\n",
            "['root vegetables', 'other vegetables', 'whole milk']->['citrus fruit'] support: 0.005795627859684799, lift: 3.0205773955773956, confidence: 0.25\n",
            "['other vegetables', 'yogurt', 'whole milk']->['pip fruit'] support: 0.005083884087442806, lift: 3.0180561692934647, confidence: 0.228310502283105\n",
            "['root vegetables', 'other vegetables', 'whole milk']->['tropical fruit'] support: 0.007015760040671073, lift: 2.884090677274582, confidence: 0.3026315789473684\n",
            "['root vegetables', 'whipped/sour cream', 'whole milk']->['other vegetables'] support: 0.005185561769191663, lift: 2.8341498143847574, confidence: 0.5483870967741935\n",
            "['root vegetables', 'yogurt', 'whole milk']->['other vegetables'] support: 0.007829181494661922, lift: 2.782852985165124, confidence: 0.5384615384615384\n",
            "['yogurt', 'whole milk', 'fruit/vegetable juice']->['other vegetables'] support: 0.005083884087442806, lift: 2.778578249396821, confidence: 0.5376344086021505\n",
            "['yogurt', 'whole milk', 'pip fruit']->['other vegetables'] support: 0.005083884087442806, lift: 2.7490189063181316, confidence: 0.5319148936170213\n",
            "['root vegetables', 'tropical fruit', 'yogurt']->['whole milk'] support: 0.0056939501779359435, lift: 2.739554317548746, confidence: 0.7\n",
            "['other vegetables', 'whipped/sour cream', 'whole milk']->['yogurt'] support: 0.005592272496187087, lift: 2.7379180839002264, confidence: 0.3819444444444444\n",
            "['other vegetables', 'whole milk', 'pip fruit']->['yogurt'] support: 0.005083884087442806, lift: 2.6948749424581857, confidence: 0.37593984962406013\n",
            "['whipped/sour cream', 'yogurt', 'whole milk']->['other vegetables'] support: 0.005592272496187087, lift: 2.6565285505915397, confidence: 0.514018691588785\n",
            "['root vegetables', 'other vegetables', 'pip fruit']->['whole milk'] support: 0.005490594814438231, lift: 2.6417130919220053, confidence: 0.675\n",
            "['tropical fruit', 'yogurt', 'whole milk']->['other vegetables'] support: 0.007625826131164209, lift: 2.601420575777561, confidence: 0.5033557046979866\n",
            "['rolls/buns', 'whole milk', 'root vegetables']->['other vegetables'] support: 0.0062023385866802234, lift: 2.5220599054125064, confidence: 0.488\n",
            "['other vegetables', 'yogurt', 'pip fruit']->['whole milk'] support: 0.005083884087442806, lift: 2.4460306406685235, confidence: 0.625\n",
            "['other vegetables', 'tropical fruit', 'yogurt']->['whole milk'] support: 0.007625826131164209, lift: 2.4258155114068005, confidence: 0.6198347107438017\n",
            "['root vegetables', 'other vegetables', 'whole milk']->['yogurt'] support: 0.007829181494661922, lift: 2.420895989974937, confidence: 0.33771929824561403\n",
            "['other vegetables', 'yogurt', 'fruit/vegetable juice']->['whole milk'] support: 0.005083884087442806, lift: 2.4158327315244676, confidence: 0.6172839506172839\n",
            "['rolls/buns', 'other vegetables', 'whole milk']->['yogurt'] support: 0.005998983223182512, lift: 2.403032235621521, confidence: 0.3352272727272727\n",
            "['root vegetables', 'other vegetables', 'whipped/sour cream']->['whole milk'] support: 0.005185561769191663, lift: 2.376144050935137, confidence: 0.6071428571428571\n",
            "['root vegetables', 'other vegetables', 'yogurt']->['whole milk'] support: 0.007829181494661922, lift: 2.3728423222863158, confidence: 0.6062992125984252\n",
            "['root vegetables', 'other vegetables', 'tropical fruit']->['whole milk'] support: 0.007015760040671073, lift: 2.2317502704942562, confidence: 0.5702479338842975\n",
            "['root vegetables', 'other vegetables', 'citrus fruit']->['whole milk'] support: 0.005795627859684799, lift: 2.187039161068327, confidence: 0.5588235294117647\n",
            "['other vegetables', 'whipped/sour cream', 'yogurt']->['whole milk'] support: 0.005592272496187087, lift: 2.152506963788301, confidence: 0.55\n",
            "['rolls/buns', 'other vegetables', 'yogurt']->['whole milk'] support: 0.005998983223182512, lift: 2.0434096679567135, confidence: 0.5221238938053098\n",
            "['rolls/buns', 'yogurt', 'whole milk']->['other vegetables'] support: 0.005998983223182512, lift: 1.9929488698614848, confidence: 0.38562091503267976\n",
            "['rolls/buns', 'other vegetables', 'root vegetables']->['whole milk'] support: 0.0062023385866802234, lift: 1.989438254410399, confidence: 0.5083333333333333\n",
            "['other vegetables', 'yogurt', 'whole milk']->['rolls/buns'] support: 0.005998983223182512, lift: 1.4646831797380422, confidence: 0.2694063926940639\n",
            "['root vegetables', 'other vegetables', 'whole milk']->['rolls/buns'] support: 0.0062023385866802234, lift: 1.4545571363455625, confidence: 0.2675438596491228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Improved Algorithm"
      ],
      "metadata": {
        "id": "sB-IoCF6P2EN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##install pyspark and create session"
      ],
      "metadata": {
        "id": "PAE1FOLu42ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NSKhiA5P5cC",
        "outputId": "24283543-dc16-439a-88cc-aed361f57d73"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 35 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 59.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=f2684d4c37fc3fa4a016aaf2c2490dc4b51b2d68d74c447ec44bee3ad1c18786\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "ZxRbLD62Qdpk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('apriori').getOrCreate()"
      ],
      "metadata": {
        "id": "54ya4KgCQgXd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "edcdb439-0358-4b43-f2dd-4af35e568d9a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ccfba209ddf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'apriori'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 147\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1586\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2647)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2644)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2734)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:95)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SPARK_CONTENT = spark.sparkContext"
      ],
      "metadata": {
        "id": "2ZNkft8AQiz5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##improved class"
      ],
      "metadata": {
        "id": "LZl-Zg2i49wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain, combinations\n",
        "\n",
        "\n",
        "class SoarkArules:\n",
        "    PARTITION_NUM = 5\n",
        "    SPARK_CONTENT = SparkSession.builder.config(\"spark.driver.memory\", \"10g\").appName('spark_apriori').getOrCreate().sparkContext\n",
        "\n",
        "    def __init__(self):\n",
        "        self.frequent_items_rdd = None\n",
        "        self.transaction_num = 0\n",
        "\n",
        "    @classmethod\n",
        "    def _first_level_process(cls, transactions_rdd, min_sup_count):\n",
        "        \"\"\"\n",
        "        get first level rdd from transactions\n",
        "        :param transactions_rdd: transactions rdd (key=set([trandaction_id]), set(items))\n",
        "        :param min_sup_count:\n",
        "        :return \n",
        "        \"\"\"\n",
        "        return transactions_rdd.flatMap(lambda x: [(frozenset([item]), frozenset(x[0])) for item in x[1]]).\\\n",
        "               reduceByKey(lambda x, y: x.union(y)).filter(lambda x: len(x[1])>=min_sup_count)\n",
        "\n",
        "    @classmethod\n",
        "    def _upper_level_process(cls,first_level_rdd, previous_level_rdd, level_num, min_sup_count):\n",
        "        \"\"\"\n",
        "        calculate current level data by joining previous_level_rdd elements\n",
        "        :param first_level_rdd:\n",
        "        :param previous_level_rdd:\n",
        "        :param level_number: what is number of current level?\n",
        "        :param min_sup_count:\n",
        "        :return: return current level rdd\n",
        "        \"\"\"\n",
        "        current_level_rdd = previous_level_rdd.cartesian(first_level_rdd)\n",
        "        current_level_rdd = current_level_rdd.map(lambda x: (x[0][0].union(x[1][0]), \n",
        "                                                                x[0][1].intersection(x[1][1]))).\\\n",
        "                                                                filter(lambda x: len(x[0])==level_num).\\\n",
        "                                                                filter(lambda x: len(x[1])>=min_sup_count)\n",
        "        return current_level_rdd\n",
        "    \n",
        "    @classmethod\n",
        "    def _get_partision_frequent_item_sets(cls, transactions_rdd, min_sup_count):\n",
        "        \"\"\"\n",
        "        get frequent items in one partition\n",
        "        :param transactions_rdd:\n",
        "        :param min_sup_count:\n",
        "        :return: all frequent_items\n",
        "        \"\"\"\n",
        "        first_level_rdd = cls._first_level_process(transactions_rdd, min_sup_count)\n",
        "        max_level = first_level_rdd.count()\n",
        "        level_num = 1\n",
        "        current_level_rdd = None\n",
        "        while True:\n",
        "            print(\"loop level\", level_num+1)\n",
        "            level_num += 1\n",
        "            if current_level_rdd is None:\n",
        "                result = cls._upper_level_process(first_level_rdd=first_level_rdd,\n",
        "                                                  previous_level_rdd=first_level_rdd, \n",
        "                                                  level_num=level_num,\n",
        "                                                  min_sup_count=min_sup_count)\n",
        "                current_level_rdd = cls.SPARK_CONTENT.union([first_level_rdd, result]).distinct()\n",
        "            else:\n",
        "                result = cls._upper_level_process(first_level_rdd=first_level_rdd,\n",
        "                                                  previous_level_rdd=current_level_rdd.filter(lambda x: len(x[0])==level_num-1), \n",
        "                                                  level_num=level_num,\n",
        "                                                  min_sup_count=min_sup_count)\n",
        "                current_level_rdd = cls.SPARK_CONTENT.union([current_level_rdd, result]).distinct()\n",
        "\n",
        "            if result.isEmpty():\n",
        "              break\n",
        "        return current_level_rdd\n",
        "    \n",
        "    @classmethod\n",
        "    def _get_sup_count(cls, rdd):\n",
        "        \"\"\"\n",
        "        calculate support_count for each data\n",
        "        :param rdd:\n",
        "        :return: rdd with (itemset, support_count) element\n",
        "        \"\"\"\n",
        "        return rdd.map(lambda x: (x[0], len(x[1])))\n",
        "    \n",
        "    @classmethod\n",
        "    def _combine_frequent_items_sup(cls, rdd, min_sup_count):\n",
        "        \"\"\"\n",
        "        compine support_count same keys and filter based on min_sup_count\n",
        "        :param rdd:\n",
        "        :param min_sup_count:\n",
        "        :return: filterd rdd with (itemset, support_count) element\n",
        "        \"\"\"\n",
        "        return rdd.reduceByKey(lambda x, y: x+y).filter(lambda x: x[1]>=min_sup_count)\n",
        "    \n",
        "    def get_frequent_item_sets(self, transactions: list, min_sup=float('-inf')) -> list:\n",
        "        \"\"\"\n",
        "        get frequent itemsets\n",
        "        :param transactions: a list of tuple(transaction_id, items)\n",
        "        :param min_sup:\n",
        "        :return: frequent itemset rdd\n",
        "        \"\"\"\n",
        "        transactions_rdd = self.SPARK_CONTENT.parallelize(transactions, self.PARTITION_NUM)\n",
        "        self.transaction_num = len(transactions)\n",
        "        # FIRDD = transactions_rdd.mapPartitions(self._get_partision_frequent_item_sets)\n",
        "        min_sup_count = int(self.transaction_num * min_sup) if min_sup != float('-inf') else 0\n",
        "        FIRDD = self._get_partision_frequent_item_sets(transactions_rdd, min_sup_count)\n",
        "        self.frequent_items_rdd = self._get_sup_count(FIRDD)\n",
        "        # self.frequent_items_rdd = self._combine_frequent_items_sup(FIRDD_plau_Sup, min_sup_count)\n",
        "        return self.frequent_items_rdd\n",
        "    \n",
        "    def get_arules(self, min_sup=float('-inf'), min_conf=float('-inf'), min_lift=float('-inf'), sort_by='lift'):\n",
        "        \"\"\"\n",
        "        get all rules and sort it\n",
        "        :param min_sup: a float between 0 and 1\n",
        "        :param min_conf: a float between 0 and 1\n",
        "        :param min_lift: a float between 0 and 1\n",
        "        :param sort_by: choices = (lift, support, confidence)\n",
        "        :return: sorted rules\n",
        "        \"\"\"\n",
        "        base_rules_format = self.frequent_items_rdd.cartesian(self.frequent_items_rdd).\\\n",
        "                            filter(lambda item:frozenset(item[0][0].union(item[1][0]))==frozenset(item[0][0])).\\\n",
        "                            cartesian(self.frequent_items_rdd).\\\n",
        "                            map(lambda x:(x[0][0], x[0][1], x[1])).\\\n",
        "                            filter(lambda item: frozenset(item[0][1])==frozenset(item[1][0].union(item[2][0])))\n",
        "        trans_num = self.transaction_num\n",
        "        rules = base_rules_format.map(lambda x: ((x[1][0], x[2][0]), \n",
        "                                                 x[0][0]/trans_num,\n",
        "                                                 x[0][0]/x[1][0],\n",
        "                                                 x[0][0]/(x[1][0]*x[2][0])))\n",
        "        rules = rules.filter(lambda x: x[1]>=min_sup and x[2]>=min_conf and x[3]>=min_lift)\n",
        "        if sort_by=='lift':\n",
        "          return rules.sortBy(lambda x: x[3])\n",
        "        elif sort_by=='support':\n",
        "          return rules.sortBy(lambda x: x[1])\n",
        "        elif sort_by=='confidence':\n",
        "          return rules.sortBy(lambda x: x[2])\n",
        "        return rules"
      ],
      "metadata": {
        "id": "3VLtoK485Dsj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = convert_csv_to_list_tuple_data(groceries_file_path, element_1_type=frozenset, elemetn_2_type=frozenset)\n",
        "spark_arules=SoarkArules()\n",
        "frequents = spark_arules.get_frequent_item_sets(transactions, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzTXcB4hb8J2",
        "outputId": "8ceead44-aaa9-4e08-9a58-b83d92b15a11"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loop level 2\n",
            "loop level 3\n",
            "loop level 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rule = spark_arules.get_arules(min_sup=0.01, min_conf=0.4)"
      ],
      "metadata": {
        "id": "q4HsXWBPWsnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rule.take(20)"
      ],
      "metadata": {
        "id": "d03CRUwCS1iR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}